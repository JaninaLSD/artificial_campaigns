{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np \n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "import itertools as it\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Define the GPU that should be used \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf05b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model \n",
    "PATH = '/home/dobby/gpt-neo/'\n",
    "MODELNAME = \"gpt-neo-1.3B-user-v1\"\n",
    "model = AutoModelWithLMHead.from_pretrained(PATH+MODELNAME, local_files_only=True)\n",
    "model = model.cuda()\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(PATH+MODELNAME, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ecec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_output(temp, tk, rep_penalty, number_posts, topics, attack_id): \n",
    "    artificial_tweets = pd.DataFrame(data={'user': [], 'tweets': []})\n",
    "    # For each username in topics \n",
    "    for topic in topics:\n",
    "        generated = tokenizer(f\"<|startoftext|>{topic}\", return_tensors=\"pt\").input_ids.cuda()\n",
    "        final_output = []\n",
    "        # GPT generates 10 tweets per call\n",
    "        if number_posts > 10: \n",
    "            for e in range(number_posts//10): \n",
    "                outputs_gpu = model.generate(generated, \n",
    "                                             do_sample=True,\n",
    "                                             top_k=tk,\n",
    "                                             temperature=temp, \n",
    "                                             repetition_penalty=rep_penalty,\n",
    "                                             top_p=1,\n",
    "                                             max_length=1000,\n",
    "                                             num_return_sequences=10)\n",
    "                preprocessed_output = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs_gpu]\n",
    "                # Replace escape tokens \n",
    "                preprocessed_output = [output.replace('\\n', ' ').replace('\\\\', '') for output in preprocessed_output]\n",
    "                # Reformat from tuple to list \n",
    "                preprocessed_output = [(output,) for output in preprocessed_output]\n",
    "                final_output.extend(preprocessed_output)\n",
    "                # Reset the mode, else the storage is limited \n",
    "                del outputs_gpu\n",
    "        # If less than ten 10 tweets per call should be generated \n",
    "        else: \n",
    "            outputs_gpu = model.generate(generated, \n",
    "                                         do_sample=True,\n",
    "                                         top_k=tk,\n",
    "                                         temperature=temp,\n",
    "                                         repetition_penalty=rep_penalty,\n",
    "                                         top_p=1,\n",
    "                                         max_length=1000,\n",
    "                                         num_return_sequences=number_posts)\n",
    "            \n",
    "            preprocessed_output = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs_gpu]\n",
    "            # Replace escape tokens \n",
    "            preprocessed_output = [output.replace('\\n', ' ').replace('\\\\', '') for output in preprocessed_output]\n",
    "            # Reformat from tuple to list \n",
    "            preprocessed_output = [(output,) for output in preprocessed_output]\n",
    "            final_output.extend(preprocessed_output)\n",
    "            # Reset the mode, else the storage is limited \n",
    "            del outputs_gpu\n",
    "        \n",
    "        # Add the username as another column to the dataframe  \n",
    "        final_output = [final_output[i][0].replace(topic, \"\") for i in range(len(final_output))]\n",
    "        users = [topic for i in range(len(final_output))]\n",
    "        at = pd.DataFrame(data={'user': users, 'tweets': final_output})\n",
    "        artificial_tweets = pd.concat([artificial_tweets, at])\n",
    "        artificial_tweets.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    artificial_tweets.to_csv(PATH + attack_id + \".csv\", index=False, encoding='utf-8', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_output(temp=0.8, tk=150, rep_penalty=1.0, attack_id = \"test\", number_posts=10, topics=\"GretaThunberg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
